import numpy as np
import pandas as pd
from sklearn.datasets import load_iris

iris= load_iris()
x = iris.data
y = iris.target

x = (x-np.mean(x,axis=0))/np.std(x,axis=0)
x= np.c_[np.ones(x.shape[0]),x] #bias term

y_onehot = np.eye(3)[y]
def softmax(z):
  exp_z = np.exp(z-np.max(z,axis=1,keepdims=True))
  return exp_z/np.sum(exp_z,axis=1,keepdims=True)

theta = np.zeros((x.shape[1],3))
lr=0.1
loss_history = []

for i in range(1000):
    z = x @ theta
    h = softmax(z)
    loss = -np.mean(y_onehot * np.log(h))
    loss_history.append(loss)
    grad = x.T @ (h - y_onehot) / x.shape[0]
    theta -= lr * grad


# Plot convergence
plt.figure()
plt.plot(loss_history)
plt.title('Gradient Descent Convergence')
plt.xlabel('Epoch')
plt.ylabel('Loss (Cross-Entropy)')
plt.show()


z = x @ theta
h = softmax(z)
y_pred = np.argmax(h, axis=1)
accuracy = np.mean(y_pred == y)
print(f"Final Parameters:\n{theta}")
print(f"Training Accuracy: {accuracy:.2%}")
